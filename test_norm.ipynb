{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "from torch.utils.data import TensorDataset,ConcatDataset,DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "from torch import nn\n",
    "from torch.nn.functional import relu\n",
    "from tqdm import tqdm\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from ekyn import SleepStageClassifier\n",
    "import numpy as np\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "device = 'cuda'\n",
    "conditions = ['PF','Vehicle']\n",
    "path_to_pt_ekyn = f'../pt_ekyn'\n",
    "path_to_pt_snezana_mice = f'../pt_snezana_mice'\n",
    "colors = {\n",
    "    'train': '#007AFF',\n",
    "    'test': '#FF9500'\n",
    "}\n",
    "ekyn_ids = sorted(set([recording_filename.split('_')[0] for recording_filename in os.listdir(path_to_pt_ekyn)]))\n",
    "snezana_mice_ids = sorted(set([recording_filename.split('.')[0] for recording_filename in os.listdir(path_to_pt_snezana_mice)]))\n",
    "print(len(ekyn_ids),ekyn_ids)\n",
    "print(len(snezana_mice_ids),snezana_mice_ids)\n",
    "\n",
    "def load_ekyn(id,condition):\n",
    "    X,y = torch.load(f'{path_to_pt_ekyn}/{id}_{condition}.pt',weights_only=False)\n",
    "    return X,y\n",
    "def load_snezana_mice(id):\n",
    "    X,y = torch.load(f'{path_to_pt_snezana_mice}/{id}.pt',weights_only=False)\n",
    "    return X,y\n",
    "def moving_average(data, window_size=10):\n",
    "    return np.convolve(data, np.ones(window_size), 'valid') / window_size\n",
    "def moving_average(data, window_size):\n",
    "    \"\"\"Calculate moving average with specified window size.\"\"\"\n",
    "    weights = np.ones(window_size) / window_size\n",
    "    return np.convolve(data, weights, mode='valid')\n",
    "\n",
    "class EEGDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, id, condition='PF', dataset='ekyn'):\n",
    "        if dataset == 'ekyn':\n",
    "            self.X,self.y = load_ekyn(id=id,condition=condition)\n",
    "        elif dataset == 'mice':\n",
    "            self.X,self.y = load_snezana_mice(id=id)\n",
    "            \n",
    "        mean = self.X.flatten().mean()\n",
    "        std = self.X.flatten().std()\n",
    "        self.X = (self.X - mean) / (std + 1e-5)\n",
    "        self.X = self.X[:,::10]\n",
    "        self.X = self.X.unsqueeze(1)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx],self.y[idx]\n",
    "def plot_training_progress(trainlossi, testlossi, train_f1s=None, test_f1s=None, ma_window_size=10, save_path='training_metrics.jpg'):\n",
    "    \"\"\"\n",
    "    Plot training progress with loss on top subplot and F1 scores on bottom subplot.\n",
    "    Both subplots share the x-axis.\n",
    "    \"\"\"\n",
    "    # Set up figure with two subplots sharing x-axis\n",
    "    fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(12, 10), sharex=True, gridspec_kw={'height_ratios': [1, 1]})\n",
    "    \n",
    "    # Setup colors with professional palette\n",
    "    colors = {\n",
    "        'train': '#0072B2',  # Professional blue\n",
    "        'test': '#D55E00'    # Professional orange\n",
    "    }\n",
    "    \n",
    "    # Calculate x-axis values\n",
    "    x_testlossi = np.array(range(len(testlossi)))\n",
    "    \n",
    "    # PLOT 1: LOSS VALUES\n",
    "    # Plot raw training loss with low opacity\n",
    "    ax1.plot(np.linspace(0, len(testlossi)-1, len(trainlossi)), trainlossi, \n",
    "             alpha=0.2, color=colors['train'], linewidth=1, label='_nolegend_')\n",
    "    \n",
    "    # Plot validation loss\n",
    "    ax1.plot(x_testlossi, testlossi, 'o-', color=colors['test'], \n",
    "             linewidth=2, label='Validation Loss')\n",
    "    \n",
    "    # Plot moving average if we have enough data\n",
    "    if len(trainlossi) > ma_window_size:\n",
    "        trainlossi_ma = moving_average(trainlossi, ma_window_size)\n",
    "        x_trainlossi_ma = np.linspace(ma_window_size-1, len(testlossi)-1, len(trainlossi_ma))\n",
    "        ax1.plot(x_trainlossi_ma, trainlossi_ma, color=colors['train'], \n",
    "                 linewidth=2, label='Training Loss (MA)')\n",
    "        \n",
    "        # Mark minimum training loss\n",
    "        min_idx = np.argmin(trainlossi_ma)\n",
    "        min_val = np.min(trainlossi_ma)\n",
    "        ax1.plot(x_trainlossi_ma[min_idx], min_val, 'o', color=colors['train'], markersize=8)\n",
    "        ax1.annotate(f'{min_val:.4f}', xy=(x_trainlossi_ma[min_idx], min_val),\n",
    "                    xytext=(5, -15), textcoords='offset points', fontsize=10,\n",
    "                    arrowprops=dict(arrowstyle='->', color=colors['train']))\n",
    "    \n",
    "    # Mark minimum test loss\n",
    "    min_test_idx = np.argmin(testlossi)\n",
    "    min_test_val = np.min(testlossi)\n",
    "    ax1.plot(x_testlossi[min_test_idx], min_test_val, 'o', color=colors['test'], markersize=8)\n",
    "    ax1.annotate(f'{min_test_val:.4f}', xy=(x_testlossi[min_test_idx], min_test_val),\n",
    "                xytext=(5, 15), textcoords='offset points', fontsize=10,\n",
    "                arrowprops=dict(arrowstyle='->', color=colors['test']))\n",
    "    \n",
    "    # Professional styling for loss subplot\n",
    "    ax1.grid(True, linestyle='--', alpha=0.7)\n",
    "    ax1.set_ylabel('Loss', fontsize=12, fontweight='bold')\n",
    "    ax1.set_title('Training Loss', fontsize=14, fontweight='bold', pad=10)\n",
    "    ax1.legend(frameon=True, framealpha=0.9)\n",
    "    \n",
    "    # PLOT 2: F1 SCORES (only if provided)\n",
    "    if train_f1s is not None and test_f1s is not None:\n",
    "        # Plot training F1 scores\n",
    "        x_train_f1s = np.array(range(len(train_f1s)))\n",
    "        ax2.plot(x_train_f1s, train_f1s, '-', color=colors['train'], \n",
    "                linewidth=2, label='Training F1')\n",
    "        \n",
    "        # Plot validation F1 scores\n",
    "        x_test_f1s = np.array(range(len(test_f1s)))\n",
    "        ax2.plot(x_test_f1s, test_f1s, 'o-', color=colors['test'], \n",
    "                linewidth=2, label='Validation F1')\n",
    "        \n",
    "        # Mark maximum training F1\n",
    "        max_train_idx = np.argmax(train_f1s)\n",
    "        max_train_val = np.max(train_f1s)\n",
    "        ax2.plot(x_train_f1s[max_train_idx], max_train_val, 'o', color=colors['train'], markersize=8)\n",
    "        ax2.annotate(f'{max_train_val:.4f}', xy=(x_train_f1s[max_train_idx], max_train_val),\n",
    "                    xytext=(5, -15), textcoords='offset points', fontsize=10,\n",
    "                    arrowprops=dict(arrowstyle='->', color=colors['train']))\n",
    "        \n",
    "        # Mark maximum validation F1\n",
    "        max_test_idx = np.argmax(test_f1s)\n",
    "        max_test_val = np.max(test_f1s)\n",
    "        ax2.plot(x_test_f1s[max_test_idx], max_test_val, 'o', color=colors['test'], markersize=8)\n",
    "        ax2.annotate(f'{max_test_val:.4f}', xy=(x_test_f1s[max_test_idx], max_test_val),\n",
    "                    xytext=(5, 15), textcoords='offset points', fontsize=10,\n",
    "                    arrowprops=dict(arrowstyle='->', color=colors['test']))\n",
    "        \n",
    "        # Professional styling for F1 subplot\n",
    "        ax2.grid(True, linestyle='--', alpha=0.7)\n",
    "        ax2.set_xlabel('Epochs', fontsize=12, fontweight='bold')\n",
    "        ax2.set_ylabel('F1 Score', fontsize=12, fontweight='bold')\n",
    "        ax2.set_title('F1 Scores', fontsize=14, fontweight='bold', pad=10)\n",
    "        ax2.set_ylim([0, 1.05])  # F1 scores are between 0 and 1\n",
    "        ax2.legend(frameon=True, framealpha=0.9)\n",
    "    else:\n",
    "        # If no F1 scores provided, just add x-label to loss subplot\n",
    "        ax1.set_xlabel('Epochs', fontsize=12, fontweight='bold')\n",
    "        \n",
    "    # Main title for the whole figure\n",
    "    fig.suptitle('Training Progress', fontsize=16, fontweight='bold', y=0.98)\n",
    "    \n",
    "    # Adjust layout and save\n",
    "    plt.tight_layout()\n",
    "    fig.subplots_adjust(hspace=0.12)  # Reduce space between subplots\n",
    "    plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "def get_dataloader(ekyn_ids,snezana_mice_ids,batch_size,shuffle):\n",
    "    return DataLoader(\n",
    "        dataset=ConcatDataset(\n",
    "            [\n",
    "                EEGDataset(id=id,condition=condition,dataset='ekyn') for id in ekyn_ids for condition in conditions\n",
    "            ]\n",
    "            +\n",
    "            [\n",
    "                EEGDataset(id=id,dataset='mice') for id in snezana_mice_ids\n",
    "            ]\n",
    "                    ),\n",
    "        batch_size=batch_size,\n",
    "        shuffle=shuffle)\n",
    "class SimpleAdaptiveNorm(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # Learnable affine parameters\n",
    "        self.scale = nn.Parameter(torch.ones(1))\n",
    "        self.shift = nn.Parameter(torch.zeros(1))\n",
    "        \n",
    "        # Small network for signal-dependent adjustment\n",
    "        self.adaptation_net = nn.Sequential(\n",
    "            nn.Linear(4, 8),  # Input: mean, std, min, max of each sample\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(8, 2)   # Output: additional scale and shift\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Extract more complex statistics than just mean and std\n",
    "        mean = x.mean(dim=2, keepdim=False)\n",
    "        std = x.std(dim=2, keepdim=False)\n",
    "        max_val = x.max(dim=2)[0]\n",
    "        min_val = x.min(dim=2)[0]\n",
    "        \n",
    "        # Combine statistics for a richer representation\n",
    "        stats = torch.stack([mean, std, max_val, min_val], dim=1).squeeze()\n",
    "        # Get adaptive adjustments\n",
    "        adjustments = self.adaptation_net(stats)\n",
    "        adaptive_scale = adjustments[:, 0].view(-1, 1, 1)\n",
    "        adaptive_shift = adjustments[:, 1].view(-1, 1, 1)\n",
    "        \n",
    "        # Apply learnable parameters plus adaptive adjustments\n",
    "        return x * (self.scale + adaptive_scale) + (self.shift + adaptive_shift)\n",
    "class SleepStageClassifier(nn.Module):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.adaptive_norm = SimpleAdaptiveNorm()\n",
    "        self.stem = nn.Conv1d(in_channels=1,out_channels=64,kernel_size=7)\n",
    "        self.convs = nn.ModuleList([nn.Conv1d(in_channels=64,out_channels=64,kernel_size=3) for _ in range(3)])\n",
    "        self.gap = nn.AdaptiveAvgPool1d(output_size=1)\n",
    "        self.fc1 = nn.Linear(in_features=64,out_features=32)\n",
    "        self.dropout = nn.Dropout(p=0.5)\n",
    "        self.fc2 = nn.Linear(in_features=32,out_features=3)\n",
    "    def forward(self,x):\n",
    "        x = self.adaptive_norm(x)\n",
    "        x = self.stem(x)\n",
    "        x = nn.functional.relu(x)\n",
    "        for conv in self.convs:\n",
    "            x = conv(x)\n",
    "            x = nn.functional.relu(x)\n",
    "        x = self.gap(x)\n",
    "        x = x.flatten(1,2)\n",
    "        x = self.fc1(x)\n",
    "        x = nn.functional.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "    \n",
    "batch_size = 1024\n",
    "ekyn_ids = ekyn_ids[:16] # tmp\n",
    "snezana_mice_ids = snezana_mice_ids[:16] # tmp\n",
    "\n",
    "train_ekyn_ids,test_ekyn_ids = ekyn_ids[:-len(ekyn_ids)//4],ekyn_ids[-len(ekyn_ids)//4:]\n",
    "print(len(train_ekyn_ids),len(test_ekyn_ids),train_ekyn_ids,test_ekyn_ids)\n",
    "train_snezana_mice_ids,test_snezana_mice_ids = snezana_mice_ids[:-len(snezana_mice_ids)//4],snezana_mice_ids[-len(snezana_mice_ids)//4:]\n",
    "print(len(train_snezana_mice_ids),len(test_snezana_mice_ids),train_snezana_mice_ids,test_snezana_mice_ids)\n",
    "\n",
    "trainloader = get_dataloader(train_ekyn_ids,train_snezana_mice_ids,batch_size,shuffle=True)\n",
    "testloader = get_dataloader(test_ekyn_ids,test_snezana_mice_ids,batch_size,shuffle=True)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = SleepStageClassifier()\n",
    "optimizer = torch.optim.AdamW(model.parameters(),lr=3e-4)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "print(device)\n",
    "print(len(trainloader) * batch_size,len(testloader) * batch_size)\n",
    "\n",
    "def calculate_f1(logits, targets):\n",
    "    \"\"\"Calculate F1 score from logits and target labels.\"\"\"\n",
    "    preds = torch.argmax(logits, dim=1)\n",
    "    f1 = f1_score(targets.argmax(dim=1).cpu().numpy(), preds.cpu().numpy(), average='macro')\n",
    "    return f1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "validation_frequency_epochs = 1\n",
    "best_dev_loss = torch.inf\n",
    "best_dev_loss_epoch = 0\n",
    "best_dev_f1 = 0\n",
    "best_dev_f1_epoch = 0\n",
    "ma_window_size = 10\n",
    "\n",
    "model.to(device)\n",
    "model.train()\n",
    "\n",
    "trainlossi = []\n",
    "testlossi = []\n",
    "train_f1s = []\n",
    "test_f1s = []\n",
    "\n",
    "for epoch in range(1000):\n",
    "    epoch_train_f1s = []  # Collect F1s for each batch in this epoch\n",
    "    epoch_train_losses = []  # Collect losses for each batch in this epoch\n",
    "    \n",
    "    for Xi, yi in tqdm(trainloader):\n",
    "        Xi, yi = Xi.to(device), yi.to(device)    \n",
    "        logits = model(Xi)\n",
    "        optimizer.zero_grad()\n",
    "        loss = criterion(logits, yi)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Calculate and store F1 for this batch\n",
    "        batch_f1 = calculate_f1(logits, yi)\n",
    "        epoch_train_f1s.append(batch_f1)\n",
    "        epoch_train_losses.append(loss.item())\n",
    "    \n",
    "    # Add average loss and F1 for this epoch\n",
    "    trainlossi.extend(epoch_train_losses)\n",
    "    train_f1s.append(np.mean(epoch_train_f1s))\n",
    "    \n",
    "    if epoch % validation_frequency_epochs == 0:\n",
    "        model.eval()\n",
    "        all_test_preds = []\n",
    "        all_test_labels = []\n",
    "        test_losses = []\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for Xi, yi in testloader:\n",
    "                Xi, yi = Xi.to(device), yi.to(device)\n",
    "                logits = model(Xi)\n",
    "                loss = criterion(logits, yi)\n",
    "                test_losses.append(loss.item())\n",
    "                all_test_preds.append(logits.argmax(dim=1))\n",
    "                all_test_labels.append(yi.argmax(dim=1))\n",
    "            all_test_preds = torch.cat(all_test_preds).cpu()\n",
    "            all_test_labels = torch.cat(all_test_labels).cpu()\n",
    "            avg_test_loss = np.mean(test_losses)\n",
    "            test_f1 = f1_score(all_test_labels, all_test_preds, average='macro')\n",
    "\n",
    "            testlossi.append(avg_test_loss)\n",
    "            test_f1s.append(test_f1)\n",
    "\n",
    "            # Track best model by loss\n",
    "            if avg_test_loss < best_dev_loss:\n",
    "                best_dev_loss = avg_test_loss\n",
    "                best_dev_loss_epoch = epoch\n",
    "                torch.save(model.state_dict(), 'best_model_by_loss.pt')\n",
    "                \n",
    "            # Track best model by F1\n",
    "            if test_f1 > best_dev_f1:\n",
    "                best_dev_f1 = test_f1\n",
    "                best_dev_f1_epoch = epoch\n",
    "                torch.save(model.state_dict(), 'best_model_by_f1.pt')\n",
    "                \n",
    "            # Call the updated plotting function with both loss and F1 data\n",
    "            plot_training_progress(\n",
    "                trainlossi,\n",
    "                testlossi,\n",
    "                train_f1s,\n",
    "                test_f1s,\n",
    "                ma_window_size,\n",
    "                'training_metrics.jpg'\n",
    "            )\n",
    "            print(f\"Epoch {epoch}: Train Loss: {np.mean(epoch_train_losses):.4f}, Test Loss: {avg_test_loss:.4f}\")\n",
    "            print(f\"Train F1: {train_f1s[-1]:.4f}, Test F1: {test_f1:.4f}\")\n",
    "            print(f\"Best Test Loss: {best_dev_loss:.4f} (Epoch {best_dev_loss_epoch})\")\n",
    "            print(f\"Best Test F1: {best_dev_f1:.4f} (Epoch {best_dev_f1_epoch})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import ConfusionMatrixDisplay,classification_report\n",
    "\n",
    "y = torch.vstack([torch.vstack([model(Xi.to(device)).softmax(dim=1).argmax(dim=1).detach().cpu(),yi.argmax(dim=1).detach().cpu()]).T for Xi,yi in trainloader])\n",
    "y_pred = y[:,0]\n",
    "y_true = y[:,1]\n",
    "print(classification_report(y_true=y_true,y_pred=y_pred))\n",
    "ConfusionMatrixDisplay.from_predictions(y_true,y_pred)\n",
    "\n",
    "y = torch.vstack([torch.vstack([model(Xi.to(device)).softmax(dim=1).argmax(dim=1).detach().cpu(),yi.argmax(dim=1).detach().cpu()]).T for Xi,yi in testloader])\n",
    "y_pred = y[:,0]\n",
    "y_true = y[:,1]\n",
    "print(classification_report(y_true=y_true,y_pred=y_pred))\n",
    "ConfusionMatrixDisplay.from_predictions(y_true,y_pred)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
