{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['A1-0', 'A1-1', 'A4-0', 'B1-0', 'B3-1', 'C1-0', 'C4-0', 'C4-1'] ['21-HET-1', '21-HET-10', '21-HET-11', '21-HET-12', '21-HET-13', '21-HET-2', '21-HET-3', '21-HET-4'] ['22-Aug-B', '22-Aug-D', '22-Aug-E', '22-Aug-F', '22-Aug-H', '22-Oct-D', '22-Oct-E', '22-Oct-G']\n",
      "6 2 ['A1-0', 'A1-1', 'A4-0', 'B1-0', 'B3-1', 'C1-0'] ['C4-0', 'C4-1']\n",
      "6 2 ['21-HET-1', '21-HET-10', '21-HET-11', '21-HET-12', '21-HET-13', '21-HET-2'] ['21-HET-3', '21-HET-4']\n",
      "6 2 ['22-Aug-B', '22-Aug-D', '22-Aug-E', '22-Aug-F', '22-Aug-H', '22-Oct-D'] ['22-Oct-E', '22-Oct-G']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from torch.utils.data import TensorDataset,ConcatDataset,DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "import seaborn as sns\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.functional import relu\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import f1_score\n",
    "import numpy as np\n",
    "\n",
    "from lib.utils import get_dataloader\n",
    "from lib.utils import SleepStageClassifier\n",
    "from lib.utils import ekyn_ids,snezana_mice_ids,courtney_ids\n",
    "from sklearn.metrics import ConfusionMatrixDisplay,classification_report\n",
    "from lib.utils import calculate_f1,plot_training_progress\n",
    "\n",
    "batch_size = 1024\n",
    "\n",
    "ekyn_ids = ekyn_ids[:8]\n",
    "snezana_mice_ids = snezana_mice_ids[:8]\n",
    "courtney_ids = courtney_ids[:8]\n",
    "print(ekyn_ids,snezana_mice_ids,courtney_ids)\n",
    "\n",
    "train_ekyn_ids,test_ekyn_ids = ekyn_ids[:-len(ekyn_ids)//4],ekyn_ids[-len(ekyn_ids)//4:]\n",
    "print(len(train_ekyn_ids),len(test_ekyn_ids),train_ekyn_ids,test_ekyn_ids)\n",
    "train_snezana_mice_ids,test_snezana_mice_ids = snezana_mice_ids[:-len(snezana_mice_ids)//4],snezana_mice_ids[-len(snezana_mice_ids)//4:]\n",
    "print(len(train_snezana_mice_ids),len(test_snezana_mice_ids),train_snezana_mice_ids,test_snezana_mice_ids)\n",
    "train_courtney_ids,test_courtney_ids = courtney_ids[:-len(courtney_ids)//4],courtney_ids[-len(courtney_ids)//4:]\n",
    "print(len(train_courtney_ids),len(test_courtney_ids),train_courtney_ids,test_courtney_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the CNN model with two layers and multi-level classifiers\n",
    "class BaselineCNN(nn.Module):\n",
    "    def __init__(self, input_channels=1, num_classes=5):\n",
    "        super(BaselineCNN, self).__init__()\n",
    "        \n",
    "        # First conv layer\n",
    "        self.conv1 = nn.Conv1d(in_channels=input_channels, out_channels=16, kernel_size=5, stride=1, padding=2)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.pool1 = nn.MaxPool1d(kernel_size=2, stride=2)  # Downsample by 2\n",
    "        \n",
    "        # Second conv layer\n",
    "        self.conv2 = nn.Conv1d(in_channels=16, out_channels=32, kernel_size=5, stride=1, padding=2)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.pool2 = nn.MaxPool1d(kernel_size=2, stride=2)  # Downsample by 2 again\n",
    "        \n",
    "        # Third conv layer\n",
    "        self.conv3 = nn.Conv1d(in_channels=32, out_channels=64, kernel_size=5, stride=1, padding=2)\n",
    "        self.relu3 = nn.ReLU()\n",
    "        self.pool3 = nn.MaxPool1d(kernel_size=2, stride=2)  # Downsample by 2\n",
    "    \n",
    "        self.head3 = nn.Sequential(\n",
    "            nn.AdaptiveAvgPool1d(1),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(64, num_classes)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        # First layer\n",
    "        x1 = self.pool1(self.relu1(self.conv1(x)))\n",
    "        \n",
    "        # Second layer\n",
    "        x2 = self.pool2(self.relu2(self.conv2(x1)))\n",
    "        \n",
    "        # Third layer\n",
    "        x3 = self.pool3(self.relu3(self.conv3(x2)))\n",
    "        out3 = self.head3(x3)  # Third prediction\n",
    "        \n",
    "        return out3\n",
    "class MultiLevelCNN(nn.Module):\n",
    "    def __init__(self, input_channels=1, num_classes=5):\n",
    "        super(MultiLevelCNN, self).__init__()\n",
    "        \n",
    "        # First conv layer\n",
    "        self.conv1 = nn.Conv1d(in_channels=input_channels, out_channels=16, kernel_size=5, stride=1, padding=2)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.pool1 = nn.MaxPool1d(kernel_size=2, stride=2)  # Downsample by 2\n",
    "        \n",
    "        # Second conv layer\n",
    "        self.conv2 = nn.Conv1d(in_channels=16, out_channels=32, kernel_size=5, stride=1, padding=2)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.pool2 = nn.MaxPool1d(kernel_size=2, stride=2)  # Downsample by 2 again\n",
    "        \n",
    "        # Third conv layer\n",
    "        self.conv3 = nn.Conv1d(in_channels=32, out_channels=64, kernel_size=5, stride=1, padding=2)\n",
    "        self.relu3 = nn.ReLU()\n",
    "        self.pool3 = nn.MaxPool1d(kernel_size=2, stride=2)  # Downsample by 2\n",
    "        \n",
    "        # Classifier heads for each latent space\n",
    "        self.head1 = nn.Sequential(\n",
    "            nn.AdaptiveAvgPool1d(1),  # Global average pooling to 1 value per channel\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(16, num_classes)\n",
    "        )\n",
    "        \n",
    "        self.head2 = nn.Sequential(\n",
    "            nn.AdaptiveAvgPool1d(1),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(32, num_classes)\n",
    "        )\n",
    "        \n",
    "        self.head3 = nn.Sequential(\n",
    "            nn.AdaptiveAvgPool1d(1),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(64, num_classes)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # First layer\n",
    "        x1 = self.pool1(self.relu1(self.conv1(x)))\n",
    "        out1 = self.head1(x1)  # First prediction\n",
    "        \n",
    "        # Second layer\n",
    "        x2 = self.pool2(self.relu2(self.conv2(x1)))\n",
    "        out2 = self.head2(x2)  # Second prediction\n",
    "        \n",
    "        # Third layer\n",
    "        x3 = self.pool3(self.relu3(self.conv3(x2)))\n",
    "        out3 = self.head3(x3)  # Third prediction\n",
    "        \n",
    "        return out1, out2, out3  # Return predictions from all levels\n",
    "    \n",
    "# model = MultiLevelCNN(input_channels=1, num_classes=3)\n",
    "model = BaselineCNN(input_channels=1, num_classes=3)\n",
    "trainloader = get_dataloader(train_ekyn_ids[:1],snezana_mice_ids=None,courtney_ids=None,batch_size=batch_size,shuffle=True,downsample=False)\n",
    "testloader = get_dataloader(train_ekyn_ids[1:2],snezana_mice_ids=None,courtney_ids=None,batch_size=batch_size,shuffle=True,downsample=False)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "optimizer = torch.optim.AdamW(model.parameters(),lr=3e-4)\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 999: Train Loss: 0.2201, Test Loss: 0.2719 | Train F1: 0.8874, Test F1: 0.7509 | Best Test Loss: 0.2571 (Ep 790), Best Test F1: 0.7771 (Ep 977): 100%|██████████| 1000/1000 [36:52<00:00,  2.21s/it]\n"
     ]
    }
   ],
   "source": [
    "validation_frequency_epochs = 1\n",
    "best_dev_loss = torch.inf\n",
    "best_dev_loss_epoch = 0\n",
    "best_dev_f1 = 0\n",
    "best_dev_f1_epoch = 0\n",
    "ma_window_size = 10\n",
    "\n",
    "model.to(device)\n",
    "model.train()\n",
    "\n",
    "trainlossi = []\n",
    "testlossi = []\n",
    "train_f1s = []\n",
    "test_f1s = []\n",
    "progressbar = tqdm(range(1000))\n",
    "for epoch in progressbar:\n",
    "    epoch_train_f1s = []  # Collect F1s for each batch in this epoch\n",
    "    epoch_train_losses = []  # Collect losses for each batch in this epoch\n",
    "    \n",
    "    for Xi, yi in trainloader:\n",
    "        Xi, yi = Xi.to(device), yi.to(device)\n",
    "        outputs = model(Xi)\n",
    "        optimizer.zero_grad()\n",
    "        if isinstance(outputs,torch.Tensor):\n",
    "            outputs = [outputs]\n",
    "        losses = [criterion(output,yi) for output in outputs]\n",
    "        loss = sum(losses)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Calculate and store F1 for this batch\n",
    "        batch_f1 = calculate_f1(outputs[-1], yi)\n",
    "        epoch_train_f1s.append(batch_f1)\n",
    "        epoch_train_losses.append(loss.item())\n",
    "    \n",
    "    # Add average loss and F1 for this epoch\n",
    "    trainlossi.extend(epoch_train_losses)\n",
    "    train_f1s.append(np.mean(epoch_train_f1s))\n",
    "    \n",
    "    if epoch % validation_frequency_epochs == 0:\n",
    "        model.eval()\n",
    "        all_test_preds = []\n",
    "        all_test_labels = []\n",
    "        test_losses = []\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for Xi, yi in testloader:\n",
    "                Xi, yi = Xi.to(device), yi.to(device)\n",
    "                outputs = model(Xi)\n",
    "                if isinstance(outputs,torch.Tensor):\n",
    "                    outputs = [outputs]\n",
    "                losses = [criterion(output,yi) for output in outputs]\n",
    "                loss = sum(losses)\n",
    "                test_losses.append(loss.item())\n",
    "                all_test_preds.append(outputs[-1].argmax(dim=1))\n",
    "                all_test_labels.append(yi.argmax(dim=1))\n",
    "            all_test_preds = torch.cat(all_test_preds).cpu()\n",
    "            all_test_labels = torch.cat(all_test_labels).cpu()\n",
    "            avg_test_loss = np.mean(test_losses)\n",
    "            test_f1 = f1_score(all_test_labels, all_test_preds, average='macro')\n",
    "\n",
    "            testlossi.append(avg_test_loss)\n",
    "            test_f1s.append(test_f1)\n",
    "\n",
    "            # Track best model by loss\n",
    "            if avg_test_loss < best_dev_loss:\n",
    "                best_dev_loss = avg_test_loss\n",
    "                best_dev_loss_epoch = epoch\n",
    "                torch.save(model.state_dict(), '../models/best_model_by_loss.pt')\n",
    "                \n",
    "            # Track best model by F1\n",
    "            if test_f1 > best_dev_f1:\n",
    "                best_dev_f1 = test_f1\n",
    "                best_dev_f1_epoch = epoch\n",
    "                torch.save(model.state_dict(), '../models/best_model_by_f1.pt')\n",
    "\n",
    "            progressbar.set_description(\n",
    "                f\"Epoch {epoch}: Train Loss: {np.mean(epoch_train_losses):.4f}, Test Loss: {avg_test_loss:.4f} | \"\n",
    "                f\"Train F1: {train_f1s[-1]:.4f}, Test F1: {test_f1:.4f} | \"\n",
    "                f\"Best Test Loss: {best_dev_loss:.4f} (Ep {best_dev_loss_epoch}), \"\n",
    "                f\"Best Test F1: {best_dev_f1:.4f} (Ep {best_dev_f1_epoch})\"\n",
    "            )\n",
    "    # Call the updated plotting function with both loss and F1 data\n",
    "    plot_training_progress(\n",
    "        trainlossi,\n",
    "        testlossi,\n",
    "        train_f1s,\n",
    "        test_f1s,\n",
    "        ma_window_size,\n",
    "        '../models/training_metrics.jpg'\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BaselineCNN(\n",
       "  (conv1): Conv1d(1, 16, kernel_size=(5,), stride=(1,), padding=(2,))\n",
       "  (relu1): ReLU()\n",
       "  (pool1): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (conv2): Conv1d(16, 32, kernel_size=(5,), stride=(1,), padding=(2,))\n",
       "  (relu2): ReLU()\n",
       "  (pool2): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (conv3): Conv1d(32, 64, kernel_size=(5,), stride=(1,), padding=(2,))\n",
       "  (relu3): ReLU()\n",
       "  (pool3): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (head3): Sequential(\n",
       "    (0): AdaptiveAvgPool1d(output_size=1)\n",
       "    (1): Flatten(start_dim=1, end_dim=-1)\n",
       "    (2): Linear(in_features=64, out_features=3, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
