{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "from torch.utils.data import TensorDataset,ConcatDataset,DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "from torch import nn\n",
    "from torch.nn.functional import relu\n",
    "from tqdm import tqdm\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "device = 'cuda'\n",
    "conditions = ['PF','Vehicle']\n",
    "path_to_pt_ekyn = f'../../pt_ekyn'\n",
    "path_to_pt_snezana_mice = f'../../pt_snezana_mice'\n",
    "\n",
    "ekyn_ids = sorted(set([recording_filename.split('_')[0] for recording_filename in os.listdir(path_to_pt_ekyn)]))\n",
    "snezana_mice_ids = sorted(set([recording_filename.split('.')[0] for recording_filename in os.listdir(path_to_pt_snezana_mice)]))\n",
    "print(len(ekyn_ids),ekyn_ids)\n",
    "print(len(snezana_mice_ids),snezana_mice_ids)\n",
    "\n",
    "def load_ekyn(id,condition):\n",
    "    X,y = torch.load(f'{path_to_pt_ekyn}/{id}_{condition}.pt',weights_only=False)\n",
    "    return X,y\n",
    "def load_snezana_mice(id):\n",
    "    X,y = torch.load(f'{path_to_pt_snezana_mice}/{id}.pt',weights_only=False)\n",
    "    return X,y\n",
    "\n",
    "class EEGDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, id, condition):\n",
    "        self.X,self.y = load_ekyn(id=id,condition=condition)\n",
    "        mean = self.X.flatten().mean()\n",
    "        std = self.X.flatten().std()\n",
    "        self.X = (self.X - mean) / (std + 1e-5)\n",
    "        # self.X = self.X[:,::10]\n",
    "        self.X = self.X.unsqueeze(1)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx],self.y[idx]\n",
    "    \n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv1d(1, 32, kernel_size=3, stride=2, padding=1)\n",
    "        self.conv2 = nn.Conv1d(32, 64, kernel_size=3, stride=2, padding=1)\n",
    "        self.convs = nn.ModuleList([nn.Conv1d(64, 64, kernel_size=3, stride=2, padding=1) for _ in range(2)])  # Reduced from 8\n",
    "        self.dropout = nn.Dropout(0.5)  # Added regularization\n",
    "        self.gap = nn.AdaptiveAvgPool1d(1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.conv1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = torch.relu(self.conv2(x))\n",
    "        x = self.dropout(x)\n",
    "        for conv in self.convs:\n",
    "            x = torch.relu(conv(x))\n",
    "            x = self.dropout(x)\n",
    "        # x = self.gap(x)\n",
    "        return x\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # self.fc1 = nn.Linear(1,313)\n",
    "        self.convs = nn.ModuleList([nn.ConvTranspose1d(64, 64, kernel_size=3, stride=2, padding=1, output_padding=1) for _ in range(2)])  # Reduced from 8\n",
    "        self.conv5 = nn.ConvTranspose1d(64, 32, kernel_size=3, stride=2, padding=1, output_padding=1)\n",
    "        self.conv6 = nn.ConvTranspose1d(32, 1, kernel_size=3, stride=2, padding=1, output_padding=1)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "\n",
    "    def forward(self, x, original_length=5000):\n",
    "        # x = self.fc1(x)\n",
    "        for conv in self.convs:\n",
    "            x = torch.relu(conv(x))\n",
    "            x = self.dropout(x)\n",
    "        x = torch.relu(self.conv5(x))\n",
    "        x = self.conv6(x)\n",
    "        if x.size(-1) != original_length:\n",
    "            x = x[:, :, :original_length] if x.size(-1) > original_length else nn.functional.pad(x, (0, original_length - x.size(-1)))\n",
    "        return x\n",
    "\n",
    "class Autoencoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.encoder = Encoder()\n",
    "        self.decoder = Decoder()\n",
    "\n",
    "    def forward(self, x):\n",
    "        original_length = x.size(-1)  # Capture input length\n",
    "        x = self.encoder(x)\n",
    "        x = self.decoder(x, original_length=original_length)\n",
    "        return x\n",
    "\n",
    "    \n",
    "traindataset = ConcatDataset([EEGDataset(id=id,condition='PF') for id in ekyn_ids[:1]])\n",
    "testdataset  = ConcatDataset([EEGDataset(id='A1-1',condition='PF')])\n",
    "\n",
    "trainloader = DataLoader(traindataset, batch_size=512, shuffle=True)\n",
    "testloader = DataLoader(testdataset, batch_size=512, shuffle=True)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "autoencoder = Autoencoder()\n",
    "optimizer = torch.optim.AdamW(autoencoder.parameters(),lr=3e-3)\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "print(device)\n",
    "\n",
    "Xi,yi = next(iter(trainloader))\n",
    "z = autoencoder.encoder(Xi)\n",
    "print(z.shape)\n",
    "autoencoder.decoder(z).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "autoencoder.to(device)\n",
    "autoencoder.train()\n",
    "    \n",
    "lossi = []\n",
    "for epoch in range(5):\n",
    "    for Xi,yi in tqdm(trainloader):\n",
    "        Xi,yi = Xi.to(device),yi.to(device)\n",
    "        logits = autoencoder(Xi)\n",
    "        optimizer.zero_grad()\n",
    "        loss = criterion(logits,Xi)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        lossi.append(loss.item())\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(lossi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "import pandas as pd\n",
    "tsne = TSNE()\n",
    "Xi,yi = next(iter(trainloader))\n",
    "Xi,yi = Xi.to(device),yi.to(device)\n",
    "Xi_tsne = tsne.fit_transform(autoencoder.encoder(Xi).flatten(1,2).detach().cpu())\n",
    "df = pd.DataFrame(torch.hstack([torch.from_numpy(Xi_tsne),yi.detach().cpu().argmax(dim=1,keepdim=True)]))\n",
    "sns.scatterplot(data=df,x=0,y=1,hue=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xi,yi = next(iter(trainloader))\n",
    "Xi,yi = Xi.cpu(),yi.cpu()\n",
    "autoencoder.cpu();\n",
    "logits = autoencoder(Xi)\n",
    "criterion(logits,Xi)\n",
    "with torch.no_grad():\n",
    "    plt.plot(Xi[0].T)\n",
    "    plt.plot(autoencoder(Xi)[0].T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set model to evaluation mode\n",
    "autoencoder.eval()\n",
    "Xi,yi = next(iter(trainloader))\n",
    "# Extract latent vectors\n",
    "with torch.no_grad():\n",
    "    eeg_data = Xi.to(device)  # Your EEG data tensor\n",
    "    latents = autoencoder.encoder(eeg_data)  # Shape: (batch_size, 128)\n",
    "\n",
    "# Use latents for further analysis (e.g., classification)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, noise_dim, output_channels=64, output_time_steps=1250):\n",
    "        super(Generator, self).__init__()\n",
    "        self.noise_dim = noise_dim\n",
    "        self.output_channels = output_channels\n",
    "        self.output_time_steps = output_time_steps\n",
    "        \n",
    "        # Define the MLP layers\n",
    "        self.fc1 = nn.Linear(noise_dim, 128)\n",
    "        self.fc2 = nn.Linear(128, 256)\n",
    "        self.fc3 = nn.Linear(256, output_channels * output_time_steps)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))  # Input: [batch_size, noise_dim]\n",
    "        x = torch.relu(self.fc2(x))  # Hidden layers\n",
    "        x = self.fc3(x)              # Output: [batch_size, 64*1250]\n",
    "        x = x.view(-1, self.output_channels, self.output_time_steps)  # Reshape to [batch_size, 64, 1250]\n",
    "        return x\n",
    "\n",
    "\n",
    "def compute_mmd(x, y, sigma=1.0):\n",
    "    def gaussian_kernel(a, b, sigma):\n",
    "        a_sq = torch.sum(a ** 2, dim=1).view(-1, 1)\n",
    "        b_sq = torch.sum(b ** 2, dim=1).view(1, -1)\n",
    "        return torch.exp(- (a_sq - 2 * torch.mm(a, b.t()) + b_sq) / (2 * sigma ** 2))\n",
    "\n",
    "    k_xx = gaussian_kernel(x, x, sigma)\n",
    "    k_yy = gaussian_kernel(y, y, sigma)\n",
    "    k_xy = gaussian_kernel(x, y, sigma)\n",
    "\n",
    "    mmd = k_xx.mean() + k_yy.mean() - 2 * k_xy.mean()\n",
    "    return mmd\n",
    "\n",
    "# Extract paradoxical latent vectors\n",
    "def extract_paradoxical_latents(autoencoder, train_loader, device='cpu'):\n",
    "    autoencoder.eval()  # Set to evaluation mode\n",
    "    paradoxical_latents = []\n",
    "\n",
    "    with torch.no_grad():  # Disable gradient computation\n",
    "        for data, labels in train_loader:\n",
    "            data, labels = data.to(device), labels.to(device)\n",
    "            labels = labels.argmax(dim=1)\n",
    "            # Filter paradoxical samples (label 0)\n",
    "            paradoxical_mask = (labels == 0)\n",
    "            if paradoxical_mask.sum() > 0:  # If there are any paradoxical samples in the batch\n",
    "                paradoxical_data = data[paradoxical_mask]  # [num_paradoxical, 1, 128]\n",
    "                latent = autoencoder.encoder(paradoxical_data)  # [num_paradoxical, 64, 32]\n",
    "                paradoxical_latents.append(latent.cpu())  # Move to CPU to save GPU memory\n",
    "\n",
    "    # Concatenate all latent vectors\n",
    "    if paradoxical_latents:\n",
    "        paradoxical_latents = torch.cat(paradoxical_latents, dim=0)  # [total_paradoxical, 64, 32]\n",
    "    else:\n",
    "        paradoxical_latents = torch.tensor([])  # Empty tensor if no paradoxical samples\n",
    "\n",
    "    return paradoxical_latents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "noise_dim = 100  # Example noise dimension\n",
    "generator = Generator(noise_dim=noise_dim)\n",
    "gen_optimizer = torch.optim.Adam(generator.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mmd_lossi = []\n",
    "generator.to(device)\n",
    "generator.train()\n",
    "\n",
    "for i in range(20):\n",
    "    paradoxical_latent = extract_paradoxical_latents(autoencoder, trainloader, device)\n",
    "\n",
    "    noise = torch.randn(512, noise_dim,device=device)\n",
    "    synthetic_latent = generator(noise).cpu()\n",
    "    mmd_loss = compute_mmd(paradoxical_latent.flatten(1,2), synthetic_latent.flatten(1,2), sigma=1.0)\n",
    "    gen_optimizer.zero_grad()\n",
    "    mmd_loss.backward()\n",
    "    gen_optimizer.step()\n",
    "    mmd_lossi.append(mmd_loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(mmd_lossi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate synthetic data\n",
    "num_synthetic = 50  # Number of synthetic samples to generate\n",
    "noise = torch.randn(num_synthetic, noise_dim, device=device)\n",
    "synthetic_latent = generator(noise)  # [num_synthetic, 64, 1250]\n",
    "synthetic_eeg = autoencoder.decoder(synthetic_latent)\n",
    "plt.plot(synthetic_eeg[0].T.cpu().detach())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def compute_mmd(source, target, kernel_sigma=1.0):\n",
    "    \"\"\"\n",
    "    Compute MMD between source and target latent features using a Gaussian kernel.\n",
    "    Args:\n",
    "        source: Tensor of shape (batch_size, feature_dim), e.g., (32, 64)\n",
    "        target: Tensor of shape (batch_size, feature_dim), e.g., (32, 64)\n",
    "        kernel_sigma: Bandwidth of the Gaussian kernel\n",
    "    Returns:\n",
    "        MMD loss (scalar)\n",
    "    \"\"\"\n",
    "    # Number of samples\n",
    "    n_source = source.size(0)\n",
    "    n_target = target.size(0)\n",
    "\n",
    "    # Compute pairwise distances\n",
    "    xx = torch.cdist(source, source, p=2) ** 2  # Source-Source distances\n",
    "    yy = torch.cdist(target, target, p=2) ** 2  # Target-Target distances\n",
    "    xy = torch.cdist(source, target, p=2) ** 2  # Source-Target distances\n",
    "\n",
    "    # Gaussian kernel: exp(-distance^2 / sigma^2)\n",
    "    scale = 2 * (kernel_sigma ** 2)\n",
    "    k_xx = torch.exp(-xx / scale)\n",
    "    k_yy = torch.exp(-yy / scale)\n",
    "    k_xy = torch.exp(-xy / scale)\n",
    "\n",
    "    # MMD: mean of kernel terms\n",
    "    mmd = k_xx.mean() + k_yy.mean() - 2 * k_xy.mean()\n",
    "    return mmd\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
